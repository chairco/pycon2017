{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-05-20 18:27:17,684] Found and registered 66 user environments.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_pull\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gym import wrappers\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./save_model'):\n",
    "    os.makedirs('./save_model')\n",
    "\n",
    "EPISODES = 2000\n",
    "RENDER = False\n",
    "MONITOR = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Only required once, envs will be loaded with import gym_pull afterwards\n",
    "# gym_pull.pull('github.com/ppaquette/gym-super-mario') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('ppaquette/SuperMarioBros-1-1-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MONITOR:\n",
    "    env = wrappers.Monitor(env, \"./mario-results\", force=True, video_callable=lambda count: count % 2 == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, action_size):\n",
    "        # environment settings\n",
    "        self.state_size = (84, 84, 4)\n",
    "        self.action_size = action_size\n",
    "        # parameters about epsilon\n",
    "        self.epsilon = 1.\n",
    "        self.epsilon_start, self.epsilon_end = 1.0, 0.10\n",
    "        self.epsilon_decay_step = 1.0 * (self.epsilon_start - self.epsilon_end) / 40000\n",
    "        # parameters about training\n",
    "        self.batch_size = 32\n",
    "        self.train_start = 100\n",
    "        self.update_target_rate = 1000\n",
    "        self.discount_factor = 0.99\n",
    "        self.memory = deque(maxlen=40000)\n",
    "        self.no_op_steps = 30\n",
    "        # build model\n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.optimizer = self.optimizer()\n",
    "\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        K.set_session(self.sess)\n",
    "\n",
    "        self.avg_q_max, self.avg_loss = 0, 0\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # if the error is in the interval [-1, 1], then the cost is quadratic to the error\n",
    "    # But outside the interval, the cost is linear to the error\n",
    "    def optimizer(self):\n",
    "        a = K.placeholder(shape=(None, ), dtype='int32')\n",
    "        y = K.placeholder(shape=(None, ), dtype='float32')\n",
    "\n",
    "        py_x = self.model.output\n",
    "\n",
    "        a_one_hot = K.one_hot(a, self.action_size)\n",
    "        q_value = K.sum(py_x * a_one_hot, axis=1)\n",
    "        error = K.abs(y - q_value)\n",
    "\n",
    "        quadratic_part = K.clip(error, 0.0, 1.0)\n",
    "        linear_part = error - quadratic_part\n",
    "        loss = K.mean(0.5 * K.square(quadratic_part) + linear_part)\n",
    "\n",
    "        optimizer = RMSprop(lr=0.01, epsilon=0.01)\n",
    "        updates = optimizer.get_updates(self.model.trainable_weights, [], loss)\n",
    "        train = K.function([self.model.input, a, y], [loss], updates=updates)\n",
    "\n",
    "        return train\n",
    "\n",
    "    # approximate Q function using Convolution Neural Network\n",
    "    # state is input and Q Value of each action is output of network\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=self.state_size))\n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1), activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dense(self.action_size))\n",
    "        model.summary()\n",
    "        return model\n",
    "\n",
    "    # after some time interval update the target model to be same with model\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    def get_action(self, history):\n",
    "        history = np.float32(history / 255.0)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            q_value = self.model.predict(history)\n",
    "            return np.argmax(q_value[0])\n",
    "\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    def remember(self, history, action, reward, next_history, dead):\n",
    "        self.memory.append((history, action, reward, next_history, dead))\n",
    "\n",
    "    # pick samples randomly from replay memory (with batch_size)\n",
    "    def train_replay(self):\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        if self.epsilon > self.epsilon_end:\n",
    "            self.epsilon -= self.epsilon_decay_step\n",
    "\n",
    "        mini_batch = random.sample(self.memory, self.batch_size)\n",
    "\n",
    "        history = np.zeros((self.batch_size, self.state_size[0], self.state_size[1], self.state_size[2]))\n",
    "        next_history = np.zeros((self.batch_size, self.state_size[0], self.state_size[1], self.state_size[2]))\n",
    "        target = np.zeros((self.batch_size, ))\n",
    "        action, reward, dead = [], [], []\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            history[i] = np.float32(mini_batch[i][0] / 255.)\n",
    "            next_history[i] = np.float32(mini_batch[i][3] / 255.)\n",
    "            action.append(mini_batch[i][1])\n",
    "            reward.append(mini_batch[i][2])\n",
    "            dead.append(mini_batch[i][4])\n",
    "\n",
    "        target_value = self.target_model.predict(next_history)\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            if dead[i]:\n",
    "                target[i] = reward[i]\n",
    "            else:\n",
    "                target[i] = reward[i] + self.discount_factor * np.amax(target_value[i])\n",
    "\n",
    "        loss = self.optimizer([history, action, target])\n",
    "        self.avg_loss += loss[0]\n",
    "\n",
    "    def load_model(self, name):\n",
    "        self.model.load_weights(name)\n",
    "\n",
    "    def save_model(self, name):\n",
    "        self.model.save_weights(name)\n",
    "\n",
    "\n",
    "# 210*160*3(color) --> 84*84(mono)\n",
    "# float --> integer (to reduce the size of replay memory)\n",
    "def pre_processing(observe):\n",
    "    observe = cv2.cvtColor(observe, cv2.COLOR_RGB2GRAY)\n",
    "    processed_observe = np.uint8(cv2.resize(observe, (84, 84)) * 255)\n",
    "    return processed_observe\n",
    "\n",
    "\n",
    "# instruction = {\n",
    "#     0: [1,0,0,0,0,0], # up\n",
    "#     1: [0,1,0,0,0,0], # left\n",
    "#     2: [0,0,1,0,0,0], # down\n",
    "#     3: [0,0,0,1,0,0], # right\n",
    "#     4: [0,0,0,0,1,0], # A\n",
    "#     5: [0,0,0,0,0,1]  # B\n",
    "#     }\n",
    "\n",
    "instruction = {\n",
    "    0: [0,1,0,0,0,0], # left\n",
    "    1: [0,0,0,1,0,0], # right\n",
    "    2: [0,0,0,0,1,0], # A\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get size of action from environment\n",
    "action_size = len(instruction)\n",
    "agent = DQNAgent(action_size)\n",
    "if os.path.exists('./save_model/Mario_DQN.h5'):\n",
    "    agent.load_model('./save_model/Mario_DQN.h5')\n",
    "\n",
    "scores, episodes, global_step = [], [], 0\n",
    "\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    dead = done\n",
    "    # 1 episode = 1 lives\n",
    "    step, score, start_life = 0, 0, 1\n",
    "    observe = env.reset()\n",
    "    prev_dist = 0\n",
    "\n",
    "    # this is one of DeepMind's idea.\n",
    "    # just do nothing at the start of episode to avoid sub-optimal\n",
    "    for _ in range(random.randint(1, agent.no_op_steps)):\n",
    "        action = 1\n",
    "        observe, _, done, info = env.step(instruction[action])\n",
    "        reward = info['distance'] - prev_dist\n",
    "        prev_dist = info['distance']\n",
    "\n",
    "    # At start of episode, there is no preceding frame. So just copy initial states to make history\n",
    "    state = pre_processing(observe)\n",
    "    history = np.stack((state, state, state, state), axis=2)\n",
    "    history = np.reshape([history], (1, 84, 84, 4))\n",
    "\n",
    "    while not done:\n",
    "            \n",
    "        if RENDER:\n",
    "            env.render()\n",
    "        global_step += 1\n",
    "        step += 1\n",
    "        \n",
    "        # skip frames\n",
    "        if step % 15 !=0:\n",
    "            observe, _, done, info = env.step(instruction[action])\n",
    "            if done:\n",
    "                agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "\n",
    "        # get action for the current history and go one step in environment\n",
    "        action = agent.get_action(history)\n",
    "        observe, _, done, info = env.step(instruction[action])\n",
    "        reward = info['distance'] - prev_dist\n",
    "        prev_dist = info['distance']\n",
    "        \n",
    "        # pre-process the observation --> history\n",
    "        next_state = pre_processing(observe)\n",
    "        next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
    "        next_history = np.append(next_state, history[:, :, :, :3], axis=3)\n",
    "\n",
    "        agent.avg_q_max += np.amax(agent.model.predict(np.float32(history / 255.))[0])\n",
    "\n",
    "\n",
    "        reward = np.clip(reward, -1., 1.)\n",
    "\n",
    "        # save the sample <s, a, r, s'> to the replay memory\n",
    "        agent.remember(history, action, reward, next_history, dead)\n",
    "        #agent.train_replay()\n",
    "        # update the target model with model\n",
    "        if global_step % agent.update_target_rate == 0:\n",
    "            agent.update_target_model()\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        # if agent is dead, then reset the history\n",
    "        if dead:\n",
    "            dead = False\n",
    "        else:\n",
    "            history = next_history\n",
    "\n",
    "        if done:\n",
    "\n",
    "            agent.avg_q_max, agent.avg_loss = 0, 0\n",
    "            \n",
    "        else:\n",
    "            print(\"\", end=\"\\r\")\n",
    "            print(\"episode:\", e, \"  step:\", step, \"  score:\", score, \"  memory length:\", len(agent.memory),\n",
    "              \"  epsilon:\", round(agent.epsilon, 4), \"  global_step:\", global_step,\"  average_q:\", round(agent.avg_q_max/float(step),4),\n",
    "              \"  average loss:\", round(agent.avg_loss/float(step), 4), end=\"\\r\")\n",
    "\n",
    "    if e % 5 == 0:\n",
    "        agent.save_model(\"./save_model/Mario_DQN.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
